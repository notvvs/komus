import asyncio
from typing import List, Callable, Optional
from bs4 import BeautifulSoup
import logging

from src.core.settings import settings
from src.parsers.base_parser import BaseParser
from src.services.state_manager import ParserState
from src.utils.http_requests import make_request, refresh_session_on_503

logger = logging.getLogger(__name__)


class StartPageParser(BaseParser):
    """–ü–∞—Ä—Å–µ—Ä —Å —Ä–µ–∫—É—Ä—Å–∏–≤–Ω—ã–º –æ–±—Ö–æ–¥–æ–º –∫–∞—Ç–µ–≥–æ—Ä–∏–π —Å –∫–ª–∞—Å—Å–æ–º categories__name, –∏—Å–ø–æ–ª—å–∑—É—é—â–∏–π httpx"""

    def __init__(self):
        self.visited = set()
        self.processed_count = 0
        self.state = None
        self.session_data = None  # –ö—ç—à–∏—Ä—É–µ–º cookies –∏ headers

    async def parse_page(self, *args, **kwargs) -> List[str]:
        """–°—Ç–∞—Ä—ã–π –º–µ—Ç–æ–¥ –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ - –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è"""
        return []

    async def parse_and_process(self, process_category_func: Callable,
                                limit_categories: Optional[int] = None,
                                state: Optional[ParserState] = None) -> int:
        """–†–µ–∫—É—Ä—Å–∏–≤–Ω–æ –Ω–∞—Ö–æ–¥–∏—Ç –∏ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ —Å –∫–ª–∞—Å—Å–æ–º categories__name"""
        self.state = state
        self.processed_count = state.total_categories_processed if state else 0
        self.visited = set(state.visited_urls) if state else set()
        self.limit_categories = limit_categories
        self.process_category = process_category_func

        # –ü–æ–ª—É—á–∞–µ–º —Å–≤–µ–∂—É—é —Å–µ—Å—Å–∏—é –≤ –Ω–∞—á–∞–ª–µ
        logger.info("üîÑ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–µ—Å—Å–∏–∏...")
        self.session_data = await refresh_session_on_503()

        # –ù–∞—á–∏–Ω–∞–µ–º —Å –≥–ª–∞–≤–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∫–∞—Ç–µ–≥–æ—Ä–∏–π
        categories_url = 'https://www.komus.ru/katalog/c/0/?from=menu-v1-vse_kategorii'

        # –ü–æ–ª—É—á–∞–µ–º –æ—Å–Ω–æ–≤–Ω—ã–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
        html = await self._get_page_html(categories_url)
        main_categories = self._extract_categories_with_class(html)

        logger.info(f"Found main categories: {len(main_categories)}")

        # –ï—Å–ª–∏ –µ—Å—Ç—å —Å–æ—Å—Ç–æ—è–Ω–∏–µ, –Ω–∞—á–∏–Ω–∞–µ–º —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω–æ–≥–æ –∏–Ω–¥–µ–∫—Å–∞
        start_index = state.current_main_category_index if state else 0

        if start_index > 0:
            logger.info(f"Continuing from category #{start_index + 1}")

        # –†–µ–∫—É—Ä—Å–∏–≤–Ω–æ –æ–±—Ö–æ–¥–∏–º –∫–∞–∂–¥—É—é –∫–∞—Ç–µ–≥–æ—Ä–∏—é
        for i in range(start_index, len(main_categories)):
            category_url = main_categories[i]

            # –û–±–Ω–æ–≤–ª—è–µ–º —Ç–µ–∫—É—â–∏–π –∏–Ω–¥–µ–∫—Å –≤ —Å–æ—Å—Ç–æ—è–Ω–∏–∏
            if self.state:
                self.state.current_main_category_index = i

            if self.limit_categories and self.processed_count >= self.limit_categories:
                logger.info(f"Category limit reached: {self.limit_categories}")
                break

            logger.info(f"Main category {i + 1}/{len(main_categories)}")

            await self._process_category_recursive(category_url)

        return self.processed_count

    async def _process_category_recursive(self, category_url: str, level: int = 0):
        """–†–µ–∫—É—Ä—Å–∏–≤–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏"""
        if category_url in self.visited:
            return

        self.visited.add(category_url)

        # –û–±–Ω–æ–≤–ª—è–µ–º —Å–æ—Å—Ç–æ—è–Ω–∏–µ
        if self.state:
            self.state.visited_urls = list(self.visited)

        indent = "  " * level

        try:
            logger.info(f"{indent}Checking: {category_url}")

            # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü—É —á–µ—Ä–µ–∑ httpx
            html = await self._get_page_html(category_url)
            soup = BeautifulSoup(html, 'html.parser')

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ —Ç–æ–≤–∞—Ä–æ–≤
            if self._has_products(soup):
                # –≠—Ç–æ –∫–æ–Ω–µ—á–Ω–∞—è –∫–∞—Ç–µ–≥–æ—Ä–∏—è - —Å—Ä–∞–∑—É –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º
                logger.info(f"{indent}Found category with products")

                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–∞ –ª–∏ —É–∂–µ —ç—Ç–∞ –∫–∞—Ç–µ–≥–æ—Ä–∏—è
                if self.state and category_url in self.state.processed_categories:
                    logger.info(f"{indent}Category already processed, skipping")
                    return

                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ª–∏–º–∏—Ç
                if self.limit_categories and self.processed_count >= self.limit_categories:
                    return

                self.processed_count += 1

                # –í—ã–∑—ã–≤–∞–µ–º —Ñ—É–Ω–∫—Ü–∏—é –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
                await self.process_category(category_url, self.processed_count)

                # –î–æ–±–∞–≤–ª—è–µ–º –≤ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ
                if self.state:
                    self.state.processed_categories.append(category_url)
                    self.state.total_categories_processed = self.processed_count

                return

            # –ï—Å–ª–∏ —Ç–æ–≤–∞—Ä–æ–≤ –Ω–µ—Ç, –∏—â–µ–º –ø–æ–¥–∫–∞—Ç–µ–≥–æ—Ä–∏–∏ —Å –∫–ª–∞—Å—Å–æ–º categories__name
            subcategories = self._extract_categories_with_class(html)

            if subcategories:
                logger.info(f"{indent}Found subcategories: {len(subcategories)}")

                for subcat_url in subcategories:
                    if self.limit_categories and self.processed_count >= self.limit_categories:
                        break

                    # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —É–∂–µ –ø–æ—Å–µ—â–µ–Ω–Ω—ã–µ URL
                    if subcat_url not in self.visited:
                        await self._process_category_recursive(subcat_url, level + 1)
            else:
                logger.info(f"{indent}No subcategories found")

        except Exception as e:
            logger.error(f"{indent}Error processing {category_url}: {e}")

            # –ü—Ä–∏ –æ—à–∏–±–∫–µ –ø—ã—Ç–∞–µ–º—Å—è –æ–±–Ω–æ–≤–∏—Ç—å —Å–µ—Å—Å–∏—é
            if "503" in str(e) or "Forbidden" in str(e):
                logger.warning(f"{indent}Session issue detected, refreshing session...")
                self.session_data = await refresh_session_on_503()

    async def _get_page_html(self, url: str) -> str:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ HTML —Å—Ç—Ä–∞–Ω–∏—Ü—ã —á–µ—Ä–µ–∑ httpx"""
        try:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º session manager - –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ —Å–µ—Å—Å–∏–µ–π
            response = await make_request(
                url=url,
                method="GET",
                max_retries=3
            )

            # –ü—Ä–∏ —É—Å–ø–µ—à–Ω–æ–º –∑–∞–ø—Ä–æ—Å–µ –æ–±–Ω–æ–≤–ª—è–µ–º —Å–µ—Å—Å–∏—é –¥–∞–Ω–Ω—ã–º–∏
            if response.status_code == 200:
                # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞ –∫–∞–∫ –≤ –æ—Ä–∏–≥–∏–Ω–∞–ª–µ
                await asyncio.sleep(settings.page_load_delay)
                await asyncio.sleep(0.5)  # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞
                return response.text
            else:
                logger.error(f"HTTP {response.status_code} for {url}")
                return ""

        except Exception as e:
            logger.error(f"Request error for {url}: {e}")
            return ""

    def _has_products(self, soup: BeautifulSoup) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –Ω–∞–ª–∏—á–∏–µ —Ç–æ–≤–∞—Ä–æ–≤ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ"""
        return bool(soup.find('div', class_='product-plain') or
                    soup.find('a', class_='product-plain__name'))

    def _extract_categories_with_class(self, html: str) -> List[str]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Å—Å—ã–ª–∫–∏ –¢–û–õ–¨–ö–û –Ω–∞ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ —Å –∫–ª–∞—Å—Å–æ–º categories__name"""
        if isinstance(html, str):
            soup = BeautifulSoup(html, 'html.parser')
        else:
            soup = html

        categories = []

        # –ò—â–µ–º –¢–û–õ–¨–ö–û —ç–ª–µ–º–µ–Ω—Ç—ã —Å –∫–ª–∞—Å—Å–æ–º categories__name
        category_links = soup.find_all('a', class_='categories__name')

        for link in category_links:
            href = link.get('href', '')
            if href and '/katalog/' in href:
                # –§–æ—Ä–º–∏—Ä—É–µ–º –ø–æ–ª–Ω—ã–π URL
                full_url = settings.base_url.rstrip('/') + href if href.startswith('/') else href
                categories.append(full_url)

        return categories